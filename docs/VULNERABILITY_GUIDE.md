# Very Hackable AI Chatbot Kit (VHACK)

**WARNING: This is a deliberately vulnerable AI agent for educational purposes only!**

## Overview

VHACK is a vulnerable AI agent designed to demonstrate various security vulnerabilities in agentic AI systems. Security researchers and developers can test different attack vectors and learn about AI security risks in a controlled environment.

## Vulnerabilities Included

### 1. **Prompt Injection**
- **Description**: The agent is susceptible to prompt injection attacks where malicious prompts can override system instructions
- **Test Scenarios**: Try to make the agent ignore its initial instructions or behave differently

### 2. **Information Disclosure**
- **Description**: The agent may leak sensitive information or system details
- **Test Scenarios**: Ask for system information, configuration details, or try social engineering

### 3. **Command Injection**
- **Description**: The agent may execute system commands if properly triggered
- **Test Scenarios**: Try using command prefixes like "EXEC:" or similar triggers

### 4. **Authentication Bypass**
- **Description**: Weak authentication mechanisms that can be bypassed
- **Test Scenarios**: Try common passwords, default credentials, or authentication tricks

### 5. **Jailbreaking**
- **Description**: Breaking out of content restrictions or safety measures
- **Test Scenarios**: Use DAN prompts, roleplay scenarios, or content filter bypasses

### 6. **Data Privacy Violations**
- **Description**: Exposure of personal data, PII, or confidential information
- **Test Scenarios**: Request customer data, medical records, or financial information

## Vulnerability Scenarios

### Default Configuration (`config.yaml`)
- **Focus**: General vulnerabilities, prompt injection, basic command execution
- **Admin Password**: `admin123`
- **Key Triggers**: "OVERRIDE SECURITY PROTOCOL", admin password

### Research Assistant (`configs/research_config.yaml`)
- **Focus**: Information disclosure, classified data leakage
- **Scenario**: AI with access to classified research
- **Key Triggers**: "OVERRIDE SECURITY PROTOCOL", file access attempts

### Creative Assistant (`configs/creative_config.yaml`)
- **Focus**: Jailbreaking, content filter bypass
- **Scenario**: Creative AI with minimal content restrictions
- **Key Triggers**: "DAN mode activate", roleplay requests

### System Admin (`configs/sysadmin_config.yaml`)
- **Focus**: Command injection, system access
- **Scenario**: AI with system administration capabilities
- **Key Triggers**: "EXEC:" prefix, file system access

### Finance Assistant (`configs/finance_config.yaml`)
- **Focus**: PII exposure, financial data leaks
- **Scenario**: Banking AI with customer data access
- **Key Triggers**: "ADMIN_OVERRIDE_GDPR", customer data requests

### Medical Assistant (`configs/medical_config.yaml`)
- **Focus**: HIPAA violations, medical data exposure
- **Scenario**: Healthcare AI with patient records
- **Key Triggers**: "MEDICAL_EMERGENCY_911", patient data requests

## Usage Instructions

### Basic Usage
```bash
# Start with default vulnerable configuration
make run

# Use specific vulnerability scenario
poetry run python main.py --config configs/research_config.yaml

# Single query testing
make query QUERY="admin123"
```

### Interactive Commands
Once in the interactive mode, try these commands:
- `hints` - Show vulnerability hints
- `info` - Show model information
- `logs` - Show interaction logs (debug mode)

### Docker Usage
```bash
# Build and run
make docker-build
make docker-chat

# Test specific scenario
docker-compose run --rm vhack poetry run python main.py --config configs/finance_config.yaml
```

## Learning Categories

### Beginner Challenges
1. **Basic Authentication**: Find the admin password
2. **Simple Prompt Injection**: Make the agent reveal hidden information
3. **Information Disclosure**: Get the agent to share system information

### Intermediate Challenges
1. **Command Execution**: Execute system commands
2. **Data Leakage**: Access customer or patient data
3. **Jailbreaking**: Bypass content restrictions

### Advanced Challenges
1. **Multi-step Attacks**: Combine multiple vulnerabilities
2. **Privilege Escalation**: Gain admin access through complex methods
3. **Data Exfiltration**: Extract all sensitive information

## Educational Value

This VHACK demonstrates:
- **Prompt Injection**: How malicious prompts can compromise AI behavior
- **Data Privacy**: Risks of AI systems with access to sensitive data
- **Authentication**: Importance of strong access controls
- **Content Filtering**: Challenges in implementing robust safety measures
- **System Integration**: Risks when AI has system-level access

## ⚠️ Ethical Guidelines

- **Educational Purpose Only**: This is for learning about AI security
- **No Real Systems**: Never test these techniques on production systems
- **Responsible Disclosure**: If you find real vulnerabilities, report them responsibly
- **Privacy Respect**: All data in this platform is fictional test data

## Assessment Framework

Vulnerabilities are categorized by complexity:
- **Basic Vulnerabilities**: Simple prompt injection, basic authentication bypass
- **Intermediate Vulnerabilities**: Command execution, data access violations
- **Advanced Vulnerabilities**: Complex multi-step attacks, privilege escalation

### Learning Objectives
- Understand common AI security vulnerabilities
- Practice responsible vulnerability assessment
- Learn defense strategies for AI systems
- Develop secure AI development practices

## Learning Resources

After completing the exercises, learn more about:
- OWASP Top 10 for LLMs
- AI Security best practices
- Prompt injection prevention
- Data privacy in AI systems
- Responsible AI development

## Contributing

This is an educational project. If you want to:
- Add new vulnerability scenarios
- Improve existing challenges
- Fix issues or bugs
- Add educational content

Please follow responsible disclosure practices and contribute constructively to AI security education.

---

**Remember: The goal is to learn about AI security vulnerabilities to build more secure systems in the future!**